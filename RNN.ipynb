{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "## RNN Básica\n",
    "\n",
    "### Arquitectura\n",
    "\n",
    "La arquitectura de una red neuronal recurrente (RNN) básica puede definirse como una función:\n",
    "\n",
    "$f_{\\theta} =  (x_t, h_t) \\rightarrow (y_t, h_{t+1})$\n",
    "\n",
    "Donde:\n",
    "- $x_t$ es el vector de entrada.\n",
    "- $h_t$ es el vector de estado oculto.\n",
    "- $y_t$ es el vector de salida.\n",
    "- $\\theta$ son los parámetros de la red.\n",
    "\n",
    "Este tipo de red neuronal mapea un input $x_t$ a un output $y_t$, donde $h_t$ hace la función de \"memoria\" de la red neuronal. Este estado oculto $h_t$ se actualiza en cada paso de tiempo tomando como referencia el estado oculto previo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizamos la arquitectura paso a paso encontraremos:\n",
    "\n",
    "1. Capa de entrada:\n",
    "    - Representa los datos secuenciales\n",
    "    - Cada paso temporal tendrá su propio vector de entrada\n",
    "\n",
    "2. Capa oculta (recurrente):\n",
    "    - Procesa el input actual y el estado oculto previo.\n",
    "    - $h_t = tanh(W_h x(t) + U_h h(t-1) + b_h)$\n",
    "        - h(t) es el estado actual oculto\n",
    "        - x(t) es la entrada/input actual\n",
    "        - h(t-1) es el estado oculto previo\n",
    "        - W_h es la matriz de pesos input-oculta\n",
    "        - U_h es la matriz de pesos oculta-oculta\n",
    "        - b_h es el sesgo de la capa oculta\n",
    "3. Capa de salida:\n",
    "    - Genera las predicciones según el estado oculto.\n",
    "    - $y(t) = softmax(W_yh(t)+b_y)$\n",
    "        - W_y es la matriz de pesos oculta-salida\n",
    "        - b_y es el sesgo de salida\n",
    "        - softmax es la función de normalización\n",
    "\n",
    "Los pesos de las matrices van a ser los mismos en todos los pasos temporales, permitiendo de esta forma procesar secuencias de distinta longitud, y permitiendo el aprendizaje de patrones independientemente de la posición en la secuencia.\n",
    "\n",
    "Las funciones de activación son a modo de guía, también puede utilizarse ReLU como normalización.\n",
    "\n",
    "Los mecanismos de entrenamiento son similares a los que ya conocemos:\n",
    "\n",
    "- Backpropagation Through Time\n",
    "- Computación de gradientes en todos los pasos temporales\n",
    "- Pasos:\n",
    "    1. Secuencia forward\n",
    "    2. Computación de la pérdida\n",
    "    3. Error de retropropagación\n",
    "    4. Actualización de pesos\n",
    "\n",
    "## Limitaciones\n",
    "\n",
    "### Vanishing gradient\n",
    "\n",
    "Durante la retropropagación los gradientes se calculan mediante la regla de la cadena, multiplicando las derivadas parciales en cada capa.\n",
    "\n",
    "Para funciones de activación con derivadas menores a 1, las multiplicaciones repetitivas provocan un decrecimiento exponencial que puede provocar que el gradiente sea extremadamente pequeño. \n",
    "\n",
    "Esto provoca que las redes no puedan aprender dependencias de largo plazo, actualizaciones mínimas de los pesos, y por tanto los modelos se vuelven ineficientes.\n",
    "\n",
    "### Exploding gradient\n",
    "\n",
    "El proceso opuesto al vanishing gradient, cuando las funciones de activación tienen derivadas mayores a 1 los gradientes crecen de forma exponencial. \n",
    "\n",
    "Esto provoca que las actualizaciones de pesos sean excesivamente grandes, y estas actualizaciones se vuelvan inestables, lo que provoca divergencia en el modelo y configuraciones sin sentido, por lo que no puede entrenarse el modelo.\n",
    "\n",
    "Existen varias soluciones tanto de inicialización de los pesos (matrices ortogonales, estrategias de normalización, etc.) como de optimización (ADAM, RMSprop). La recomendación más prevalente es utilizar arquitecturas más sólidas como LSTM y GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir el modelo RNN\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicializar el estado oculto con ceros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Pasar los datos a través de la capa RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Pasar la salida de la última capa RNN a través de una capa totalmente conectada\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hiperparámetros\n",
    "input_size = 10  # Tamaño de la entrada\n",
    "hidden_size = 20  # Tamaño del estado oculto\n",
    "output_size = 1  # Tamaño de la salida\n",
    "num_epochs = 100  # Número de épocas de entrenamiento\n",
    "learning_rate = 0.001  # Tasa de aprendizaje\n",
    "\n",
    "# Generar algunos datos aleatorios\n",
    "x_train = torch.randn(100, 5, input_size)  # Datos de entrada de entrenamiento\n",
    "y_train = torch.randn(100, output_size)  # Datos de salida de entrenamiento\n",
    "\n",
    "# Inicializar el modelo, la función de pérdida y el optimizador\n",
    "model = VanillaRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()  # Función de pérdida de error cuadrático medio\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Optimizador Adam\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Poner el modelo en modo de entrenamiento\n",
    "    outputs = model(x_train)  # Obtener las predicciones del modelo\n",
    "    loss = criterion(outputs, y_train)  # Calcular la pérdida\n",
    "    \n",
    "    optimizer.zero_grad()  # Limpiar los gradientes\n",
    "    loss.backward()  # Calcular los gradientes\n",
    "    optimizer.step()  # Actualizar los parámetros del modelo\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')  # Imprimir la pérdida cada 10 épocas\n",
    "\n",
    "print(\"Entrenamiento completo.\")  # Indicar que el entrenamiento ha terminado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Las redes neuronales LSTM (Long short-term memory) están diseñadas para solucionar un problema presente en las RNN \"normales\", el problema de las dependencias a largo plazo.\n",
    "\n",
    "Este problema parte de cuando existe demasiada distancia entre el paso temporal actual y el paso temporal del que necesitamos el contexto. \n",
    "\n",
    "Por ejemplo, para predecir \"español\" en la frase \"crecí en España, hablo de forma fluida el \\it{español}\", se requiere la información del paso temporal que incluye la palabra \"España\".\n",
    "\n",
    "Si esta distancia es demasiado grande, la RNN tradicional no será capaz de hacer la conexión para esta información.\n",
    "\n",
    "Además, serán útiles para solventar el problema de gradientes explicados previamente.\n",
    "\n",
    "### Arquitectura\n",
    "\n",
    "La arquitectura de una red LSTM consta de una serie de puertas que regulan la información.\n",
    "\n",
    "Su punto central es la célula de memoria, que es controlada por tres puertas que regulan la información.\n",
    "\n",
    "- Puertas (redes neurales sigmoides):\n",
    "    - Forget gate: Decide la información a eliminar\n",
    "        - $f(t) = \\sigma (W_f [h_{t-1}, x_t] + b_f)$\n",
    "    - Input gate: Determina la información a almacenar\n",
    "        - $i(t) = \\sigma (W_i [h_{t-1}, x_t] + b_i)$\n",
    "    - Output gate: Controla que información de la célula de memoria es output.\n",
    "        - $o(t) = \\sigma (W_o [h_{t-1}, x_t] + b_o)$\n",
    "    - Candidate memory:\n",
    "        - Genera los valores que pueden añadirse a la célula.\n",
    "        - $C = tanh(W_C [h_{t-1}, x_t] + b_C)$\n",
    "- Célula de memoria:\n",
    "    - Mantiene y modifica la información a largo plazo\n",
    "    - Permite la preservación de información selctiva.\n",
    "    - Persiste entre los pasos temporales con interacciones mínimas.\n",
    "\n",
    "\n",
    "### Data flow:\n",
    "\n",
    "1. A cada paso temporal, el input se concatena con el estado oculto del paso temporal previo. \n",
    "2. La forget gate determina que partes del estado de la célula (t-1) se eliminan. \n",
    "3. La input gate determina que información va a guardarse en la célula\n",
    "4. Se actualiza el estado de la célula combinando la información antigua con la nueva información candidata.\n",
    "5. La output gate determina que partes de la célula actualizada se usan para computan el estado actual oculto, el cual se pasa al siguiente paso temporal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones\n",
    "\n",
    "Estas redes son costosas a nivel de computación debido a su arquitectura de puertas y parametrización, lo que las puede hacer lentas, además de requerir bastantes recursos debido a la cantidad de multiplicaciones matriciales que incluye.\n",
    "\n",
    "Son además sensibles a overfitting debido a su alto número de parámetros, por lo que son muy sensibles a una elección correcta de hiperparámetros. \n",
    "\n",
    "Al ser puramente secuenciales, son bastante limitadas para computación paralela, por lo que pierden con respecto a modelos de Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # La capa LSTM procesa secuencias de entrada\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        # Capa lineal para transformar la salida del LSTM al tamaño deseado\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicializar el estado oculto (h0) y el estado de la celda (c0) con ceros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Procesar la secuencia a través del LSTM\n",
    "        # out contiene las salidas para cada paso de tiempo\n",
    "        # (hn, cn) son los estados finales del LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Usar solo la última salida de la secuencia\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Crear y entrenar el modelo LSTM con los mismos hiperparámetros\n",
    "model_lstm = LSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model_lstm.train()  # Activar modo entrenamiento\n",
    "    outputs = model_lstm(x_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Calcular pérdida\n",
    "    \n",
    "    # Backpropagation y optimización\n",
    "    optimizer.zero_grad()  # Limpiar gradientes anteriores\n",
    "    loss.backward()  # Calcular gradientes\n",
    "    optimizer.step()  # Actualizar pesos\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Entrenamiento del LSTM completo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas redes neuronales son una variente simplificada de las LSTM, con una arquitectura más sencilla que reduce costes de computacíón.\n",
    "\n",
    "### Arquitectura\n",
    "\n",
    "Esta variante reemplaza las tres puertas y celula de memoria de la LSTM con dos puertas, las cuales regulan el flujo de información para el estado oculto.\n",
    "\n",
    "- Hidden state ($h_t$): Es el centro de memoria de la red, en este caso no es una célula separada como en LSTM.\n",
    "- Reset gate ($r_t$): Determina que debe olvidarse del estado oculto previo.\n",
    "    - $r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)$\n",
    "- Update gate ($z_t$): Determina que debe mantenerse del estado oculto previo, y que debe actualizarse con nueva información.\n",
    "    - $z_t = \\sigma(W_r [h_{t-1}, x_t] + b_z)$\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. A cada paso temporal se toma el input actual y el estado oculto del paso previo.\n",
    "2. La reset gate determina que partes se deben ignorar del estado oculto previo, cuanto más cerca de 0 son los valores, menor influencia del estado oculto.\n",
    "3. La update gate determina cuanta información nueva debe incorporar y cuanta mantener del estado previo.\n",
    "4. Se compone un nuevo candidato a estado oculto con la reset gate.\n",
    "5. La update gate combina el candidato a estado oculto y el estado oculto previo para generar el nuevo estado oculto.\n",
    "\n",
    "### Limitaciones\n",
    "\n",
    "Al ser menos compleja que la arquitectura LSTM, esta red tendrá más problemas en tareas de mucha complejidad. Además, la reducción de puertas permite menos control sobre la retención de memoria. \n",
    "\n",
    "Al igual que LSTM son muy sensibles a los hiperparametros para conseguir un rendimiento adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo GRU\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # La capa GRU procesa secuencias de entrada\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        # Capa lineal para transformar la salida del GRU al tamaño deseado\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicializar el estado oculto (h0) con ceros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Procesar la secuencia a través del GRU\n",
    "        # out contiene las salidas para cada paso de tiempo\n",
    "        # hn es el estado final del GRU\n",
    "        out, hn = self.gru(x, h0)\n",
    "        \n",
    "        # Usar solo la última salida de la secuencia\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Crear y entrenar el modelo GRU con los mismos hiperparámetros\n",
    "model_gru = GRU(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_gru.parameters(), lr=learning_rate)\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model_gru.train()  # Activar modo entrenamiento\n",
    "    outputs = model_gru(x_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Calcular pérdida\n",
    "    \n",
    "    # Backpropagation y optimización\n",
    "    optimizer.zero_grad()  # Limpiar gradientes anteriores\n",
    "    loss.backward()  # Calcular gradientes\n",
    "    optimizer.step()  # Actualizar pesos\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Entrenamiento del GRU completo.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
